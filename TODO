OK add configuration scheduler in name of config .sh

OK configurable via sbatchman init and asked at installation

OK add to sbatchman configure option to pass modules to be loaded

OK Add preprocess/postprocess to lob launch command

# SBatchman commands
configure
  file or parameters
launch (runs a job if there does not exist a combination (cluster_name, config, tag, command) in experiments directory or if status is FAILED, TIMEOUT, CANCELLED)
  file (+ whitelist/blacklist) or parameters
archive
  by tag/config/cluster_name
update-jobs
  update status of all jobs in experiments directory
  query scheduler for status of all jobs
init
  creates global SbatchMan configuration file (cluster_name, scheduler)
status
  user interface

# API
get_jobs(name: str, archive: False) -> List[Job]
get_jobs_dataframe() -> pd.DataFrame
parse_command(command: str) -> Dict[str, Any]
create_config(name: str, scheduler: str, **kwargs) -> None
launch_job(cluster_name: str, name: str, tag: str, config: str, command: str, archive_name: str = None) -> None

Job class
 - status
 - timestamp
 - scheduler
 - cluster_name
 - tag
 - archive_name
 - status -> Status
 - config -> Config

Config class
 - name
 - scheduler
 - env variables
 - custom parameters of each scheduler

Status enum
 - PENDING
 - RUNNING
 - COMPLETED
 - FAILED
 - CANCELLED
 - UNKNOWN

.filter(lamda x: x.cluster_name = 'cluster_name' and x.tag = 'smallGraph' and archive_name = '1')[0].get_stdout() -> String
.filter(lamda x: x.cluster_name = 'cluster_name')[0].get_stderr() -> String


add command to update the status of all jobs, that queries the specific scheduler


#### generated by sbatchman
#### do not edit it manually, use sbatchman configure --file config.yaml to add configurations
baldo:
  scheduler: slurm
  configs:
    bfs1cpu:
      partition: short
      cpu: 1
      ...
    bfs2cpu:
      partition: long
      cpu: 2
      ...
hpc-unitn:
  scheduler: pbs
  default_conf:
      partition: unitn-short
      ...
  configs:
    bfs1cpu:
      cpu: 1
      <!-- default queue: unitn-short -->
      ...
    bfs2cpu:
      partition: long
      cpu: 2
      ...

# Example
sbatchman configure --file config.yaml

sbatchman configure slurm --name cpu1 --cluster-name baldo --partition short --cpu 1
sbatchman configure slurm --name cpu2 --cluster-name baldo --partition short --cpu 2
sbatchman configure pbs --name cpu1 --cluster-name hpc-unitn --partition unitn-short --cpu 1


# Archive utility for sbatchman
sbatchman archive --cluster-name baldo --experiment bfs1cpu --tag shortDiam --archive_name run1

All parameters optional

Experiments directory structure:
experiments/
├── baldo
│   ├── bfs1cpu
│   │   ├── shortDiam
│   │   │   ├── job_id
│   │   │   ├── job_id
│   │   ├── largeDiam
│   │   │   ├── job_id
│   │   │   ├── job_id
│   ├── bfs2cpu
│   │   ├── tag1
│   │   │   ├── job_id
│   │   │   ├── job_id
│   │   ├── tag2
│   │   │   ├── job_id
│   │   │   ├── job_id
archive/
|── archive_name
|   |── baldo
|   |   ├── bfs1cpu
|   |   │   ├── shortDiam
|   |   │   │   ├── job_id
|   |   │   │   ├── job_id
|   |   ├── bfs2cpu


# launch jobs
sbatchman launch --cluster-name baldo --name bfs1cpu --tag shortDiam --config bfs1cpu --command "python script.py" --archive_name run1


sbatchman launch -f config.yaml --exclude_tag shortDiam --include_config bfs1cpu,bfs2cpu
# also include/exclude the other way around

-- OK BATCH JOBS file
variables:
  merged: [True, False]
  atomics: [True, False]
  atomics: [True, False]
  scale: scales.txt
command: python script.py --file {dataset} --n {n} --m {m}
experiments:
  bfs_1_cpu:
    command: python script.py --file {dataset} --n {n} --m {m}
    experiments:
      shortDiam:
        variables:
          dataset: datasets/small
      largeDiam:
        variables:
          dataset: datasets/large
      atomics:
        command: python script.py --file {small_dim} --n {n} --m {m} --atomic
-- END OK BATCH JOBS file


document pre and post processing commands